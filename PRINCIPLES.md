# Fundamentals‑First Architecture Principles

This document is a tech‑agnostic, MECE set of architecture and design principles aimed at avoiding “organizationally neat but computationally wasteful” systems. It’s inspired by and references:

- **kellabyte — “Architecture by Fashion, Not Fundamentals: The Rise of Pattern Driven Development” (Sep 11, 2025)**  
  <https://substack.com/inbox/post/173391778>

| Principle                              | What this means in practice                                                                                                                                                                                      |
| -------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Evidence over fashion**              | Define workload model (QPS, p95 targets, data sizes); run a spike with realistic data and capture numbers; record the choice in an ADR with success criteria and a review date.                                  |
| **Data shapes drive design**           | List top access patterns (reads/writes, frequency, shapes, cardinality); draw module boundaries around data ownership and change cadence; plan schema/versioning and migrations up front.                        |
| **Put compute near the data**          | Push filters/joins/aggregations to the engine holding the data; return only needed fields; materialize precomputed views/results for expensive, stable computations.                                             |
| **Respect algorithmic complexity**     | Annotate hot paths with expected time/space complexity; choose structures accordingly (hash for point lookups, ordered for ranges, heap for top-N); replace per-item loops with batching/vectorized ops.         |
| **Index-aware thinking**               | For each hot query, specify predicate, order/grouping and the index (key order, selectivity) that supports it; ensure index/derivative updates are in the same write boundary; periodically drop unused indexes. |
| **No “post-query table scans”**        | Avoid broad fetches then filtering in memory; express business rules as predicates/joins/group-bys; if client-side is unavoidable, build explicit in-memory indexes/bitsets and stream in chunks.                |
| **Correctness before cleverness**      | State invariants and transaction boundaries; keep derived data/indexes consistent and atomic with source updates; design idempotent commands and test concurrency/crash recovery paths.                          |
| **Minimize distribution**              | Count network hops in the hot path and justify each; collocate tightly coupled components; avoid chatty protocols—prefer coarse-grained calls with clear ownership.                                              |
| **Design for flow control**            | Set concurrency limits and bounded queues; implement backpressure, batching under load, and timeouts/retries; monitor queue length, wait time, and shed load gracefully.                                         |
| **Optimize the hot path early enough** | Name the top 1–3 revenue/retention flows and set SLOs (p95/p99); build a realistic dataset and perf tests; gate releases on meeting SLOs at target scale.                                                        |
| **Make costs visible**                 | Dashboard cost/request and cost/user (CPU ms, memory, I/O, egress, \$); tag resources by service/feature; include cost impact in ADRs with budgets/alerts.                                                       |
| **Stable APIs, evolving internals**    | Keep contracts backward compatible (version only when needed); use adapters/shims so internals (indexes, batching, layouts) can change without client churn; enforce with contract tests.                        |
| **Pragmatism over ideology**           | Prefer the smallest change that meets SLOs and cost goals; if you can’t move logic to storage, bring structures (caches/indexes) to the app; plan incremental rollouts that fit team skills.                     |
| **Operational simplicity**             | Fewer services/stores with clear ownership; standardize observability/runbooks/retry policies; design for graceful degradation, kill-switches, and simple failure modes.                                         |
| **Measure, don’t assume**              | Capture before/after benchmarks on the same dataset; trace across boundaries (p50/p95/p99, CPU time, allocations, I/O waits); treat “should be fine” as a hypothesis with pass/fail criteria.                    |
| **Reversibility & guardrails**         | Use feature flags, canaries, and staged rollouts; dual-write/read during migrations with automated comparisons; keep fast rollback and scoped blast-radius limits (quotas, circuit breakers).                    |

---

## Principles Table

> Use this table as a review checklist during design docs, PRs, and performance reviews.

| Principle (Category → Principle) | What it means | Good example (concrete) | Anti-pattern (smell) | How to apply & measure |
|---|---|---|---|---|
| **Decision discipline → Evidence over fashion** | Pick designs from workloads, SLOs, and cost—not trends. | Checkout targets **P95 ≤ 200 ms**, **500 RPS**, **≤$0.003/req**; choose single-process app + relational store (≤50 GB) because joins and low hop count fit the budget. | “Microservices because industry,” no latency/cost goals. | ADRs include users/flows, volumes, SLOs, cost model, rollback criteria. Track P95 + $/req weekly. |
| **Decision discipline → Stable APIs, evolving internals** | Keep contracts steady; improve internals freely. | Keep `/reports` API; move from in-app loops → pushed-down SQL + materialized view; clients unchanged. | Public contract churn to match refactors. | Version sparingly; dual-run old/new behind a flag until result delta <0.1%. |
| **Decision discipline → Pragmatism over ideology** | Meet teams where they are; smuggle fundamentals in. | Add client-side **range index + prefix trie** under ORM to cut 9 min → 6 s without code churn. | “ORM-only” dogma causing N+1 and array scans. | Define end-state and interim step; time-box the stopgap; measure delta (latency/CPU/memory). |
| **Decision discipline → Reversibility & guardrails** | Make risky changes easy to undo. | New planner + index behind feature flag; shadow-run on 1%; canary to 10%; pre-verified rollback. | Big-bang Friday migrations; no rollback. | Every risky change: flag, canary plan, automated rollback, backup/restore drill results. |
| **Decision discipline → Performance gates in CI/CD** | Treat performance budgets as merge criteria. | CI runs perf suite on hot endpoints; fail if **P95/CPU-ms/bytes** regress >10%. | “We’ll test performance later.” | Keep stable perf harness + golden traces; store budgets in repo; block merges on regression. |
| **Data & compute placement → Data shapes drive design** | Let access patterns define boundaries and APIs. | Reporting needs *orders-by-customer-by-month* → ledger table + monthly rollup keyed `(customer_id, yyyy_mm)` owned by reports module. | Splitting entities across services that are always read together. | For each feature: top-N queries with filters/group/sort shapes. Align ownership to those shapes. |
| **Data & compute placement → Put compute near the data** | Run filters/joins/aggregates where bytes live. | Promotions: push `WHERE created_at >= NOW()-90d` + indexed joins; return **200 rows**, not **50k**. | Pull 50k rows, then filter/group in app. | For each endpoint, document “where compute runs.” Require pushdown unless justified. |
| **Data & compute placement → No post-query table scans** | Don’t fetch broad sets to re-filter later. | Search sends q/filters/sort; store returns pre-ranked, paged results. | `.ToList()` then `.Where().GroupBy().OrderBy()` in app. | Server-side row caps; lint for select-all-then-filter; alert on queries returning >5k rows. |
| **Data & compute placement → Data movement budget** | Cap bytes moved on hot paths; kill wasteful hops. | Hot endpoint budget: **≤64 KB @ P95** payload; any hop >128 KB requires ADR. | Chatty multi-hop calls moving MBs of JSON each request. | Track bytes/request (in/out). Add “payload exceeded” alert & traces showing hop sizes. |
| **Data & compute placement → Payload & serialization discipline** | Control over-fetching and codec overhead. | Lean DTO; field projection; switch JSON → compact schema (e.g., binary/JSON-GZIP) for large internal traffic; save **120 ms P95**. | DTOs mirror domain graphs; massive nested JSON everywhere. | Define response schemas; track P50/P95 payload; profile (de)serialization time in traces. |
| **Storage leverage → Storage engine feature awareness** | Use the machine you paid for before rebuilding it. | Use **covering indexes**, **partitioning**, **materialized views**, **columnar** for analytics instead of app loops. | Re-implementing ranking/window functions in code. | “Engine capability checklist” in design docs; justify bypassing native features. Observe scan/seek mix. |
| **Algorithmic efficiency → Respect algorithmic complexity** | Choose structures that meet time/space budgets. | Feed merge with **min-heap**: O(n log K); rate-limit with **ring buffer** O(1). | O(n²) duplicate detection with nested loops. | State target complexity for each hot op; microbench in CI. |
| **Algorithmic efficiency → Index-aware thinking** | Every hot query has a predicate/sort + matching index. | “Orders for customer in last 90d sorted by total desc” → composite `(customer_id, created_at desc)` + covering `total`. | Full scans then sort on every call. | Maintain “Index Map”: query → index/structure, selectivity, expected rows; alert when scans creep. |
| **Algorithmic efficiency → Locality & memory layout** | Favor contiguous data and cache-friendly access. | **Struct-of-arrays** for analytics; pre-allocate buffers; cap allocations/request; avoid pointer-chasing graphs. | Allocation churn and pointer-heavy trees on hot paths. | Track allocations/request; GC pause budgets; flamegraphs show cache-miss dominated code. |
| **Algorithmic efficiency → Set-based & vectorized work** | Prefer set operations and vectorized libs over scalar loops. | Batch **256–1k** items; vectorized readers; SIMD string match; columnar aggregations for reports. | Row-by-row processing in app for analytics. | Define default batch sizes; measure throughput delta; require vectorization support when choosing libs. |
| **Boundaries & distribution → Minimize distribution** | Prefer function calls over network hops; split only for measured reasons. | Keep admin/reporting co-located until CPU>60% or deploy-cadence conflict is proven; extract with clear benefit. | 15 services for CRUD; P95 jumps 120 ms → 450 ms. | Count hops in hot paths; budget ≤2. Merge back if P95 regresses. |
| **Boundaries & distribution → N+1 & fan-out budgets** | Bound calls per entity and per request. | Graph API enforces **≤1 backend call per entity**; aggregate endpoints **fan-out ≤ N** with batching. | ORM-driven N+1 queries; per-item downstream calls. | N+1 detector in traces; budget check in review; fail CI when exceeded in benchmarks. |
| **Flow & concurrency → Design for flow control** | Make backpressure, batching, retries explicit. | Email service batches **max(500, 50 ms)**; token buckets; producers shed non-critical work when lag > threshold. | Unbounded queues; infinite retries. | Define max queue sizes, retry budgets, and lag SLOs; alert on **queue age P95**, not count. |
| **Flow & concurrency → Concurrency & scheduling budgets** | Bound in-flight work to cores/IO; avoid stampedes. | Thread pools sized to cores; async fan-out bounded; per-tenant work queues with fairness. | Unbounded async tasks; threadpool exhaustion. | Track in-flight ops, context switches, and queue time; enforce per-tenant concurrency caps. |
| **Flow & concurrency → Skew & hot-key resilience** | Design for power-law traffic and heavy hitters. | Per-tenant quotas; shard by `(tenant_id, hash)`; hot-key detection diverts to dedicated cache. | Single shard melted by top customer. | Monitor P99/P999 by principal; autoscale/partition on skew signals; rate-limit heavy keys. |
| **Correctness & caching → Correctness before cleverness** | Writes atomic; derived data stays in sync. | Orders + inventory + ledger atomic or saga with idempotency + compensation; views versioned/read-gated. | Indexes updated async while claiming ACID. | Document txn boundaries; invariants in CI; periodic consistency checker. |
| **Correctness & caching → Caching with correctness** | Cache taxonomy + invalidation strategy with tests. | Read-through cache with **dogpile protection**; event invalidation; **negative caching** for 404s. | TTL-only cache causing stale reads of critical data. | Cache correctness tests in CI; hit/miss & staleness metrics; shadow read to verify correctness. |
| **Correctness & caching → Read/Write amplification budgets** | Track extra reads/writes from denorm/materialized views. | Product update writes base + 2 views (**amplification ≤3x**); reads served in **≤2** touches. | 8 derived tables per write; 5 downstream reads per simple view. | Set max amplification; alarm when exceeded; compute in traces. |
| **Correctness & caching → Fast-path / slow-path split** | Optimize the common case with a safe fallback. | Precomputed aggregates for 95% of queries; rare edge-cases recompute on demand. | One slow general path for all cases. | Route traffic by case; measure fast-path hit rate; ensure correctness parity tests. |
| **Cost & visibility → Make costs visible** | Know $/req and drivers; design for TCO. | Dashboard shows CPU-ms/req, egress KB/req, DB I/O/req, $/1k calls; columnar analytics cuts report cost 70%. | Autoscaling hides inefficiency; bills scale linearly with traffic. | Add “cost” panel next to latency; set cost SLOs (e.g., ≤$3/1k calls); monthly cost reviews tied to code. |
| **Cost & visibility → Measure, don’t assume** | Instrument traces/profiles to see time/bytes. | Trace shows 60% in JSON (de)serialization; schema change saves 120 ms P95. | “DB is slow” by assumption; weeks lost. | Require trace screenshots in perf PRs; keep perf harness per hot endpoint; store flamegraphs. |
| **Hot path focus → Optimize the hot path early enough** | Treat top 1–3 flows as risks now, not later. | Onboarding load **P95 ≤ 300 ms**; prototype with realistic data (100k users, 1k RPS) pre-GA; fix layout before launch. | “We’ll optimize later”; launch at 2 s P95; churn spikes. | Identify hot flows in PRD; pre-GA scale test; track P50/P95, bytes moved, CPU/request from day 1. |
| **Operations → Operational simplicity** | Fewer parts, clear failure modes, graceful degrade. | One DB + read replica, one cache, one worker pool; idempotent handlers; read-only mode on write failure. | Bespoke queues/caches per service; unclear failure propagation. | In review, delete something; prefer shared library over new service; GameDays to verify degrade modes. |

---

### How to use this file

- Paste the table into design docs as a checklist.  
- During reviews, pick 3–5 rows most relevant to the change and record explicit pass/fail notes.  
- Keep the budgets (latency, bytes, CPU, cost) alongside the service’s SLOs and alert thresholds.

